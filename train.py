import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import os
import sys

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from attention import MultiHeadAttention
from feed_forward import PositionWiseFFN
from layers import SublayerConnection
from positional_encoding import PositionalEncoding

class SimpleTransformer(nn.Module):
    """ç®€å•çš„Transformeræ¨¡å‹ï¼ˆåªæœ‰Encoderï¼‰"""
    
    def __init__(self, vocab_size, d_model=128, n_heads=4, d_ff=512, n_layers=2, max_len=1000):
        super().__init__()
        self.d_model = d_model
        
        # è¯åµŒå…¥
        self.embedding = nn.Embedding(vocab_size, d_model)
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        # Transformerå±‚
        self.layers = nn.ModuleList([
            self._make_layer(d_model, n_heads, d_ff) 
            for _ in range(n_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(d_model, vocab_size)
        
    def _make_layer(self, d_model, n_heads, d_ff):
        # åˆ›å»ºå•ä¸ªEncoderå±‚
        self_attn = MultiHeadAttention(d_model, n_heads)
        ffn = PositionWiseFFN(d_model, d_ff)
        
        return nn.ModuleDict({
            'self_attn': SublayerConnection(d_model, 0.1),
            'ffn': SublayerConnection(d_model, 0.1),
            'self_attn_module': self_attn,
            'ffn_module': ffn
        })
        
    def forward(self, x):
        # x: [batch_size, seq_len]
        
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        
        # é€šè¿‡æ‰€æœ‰Transformerå±‚
        for layer in self.layers:
            # è‡ªæ³¨æ„åŠ›å­å±‚
            x = layer['self_attn'](x, lambda x: layer['self_attn_module'](x, x, x)[0])
            # å‰é¦ˆå­å±‚
            x = layer['ffn'](x, layer['ffn_module'])
            
        # è¾“å‡ºé¢„æµ‹
        output = self.output_layer(x)
        return output

class SimpleDataset(Dataset):
    """ç®€å•çš„æ–‡æœ¬æ•°æ®é›†"""
    
    def __init__(self, texts, vocab, seq_length=20):
        self.texts = texts
        self.vocab = vocab
        self.seq_length = seq_length
        
    def __len__(self):
        return len(self.texts) - self.seq_length
        
    def __getitem__(self, idx):
        # è·å–è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—ï¼ˆè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼‰
        input_seq = self.texts[idx:idx + self.seq_length]
        target_seq = self.texts[idx + 1:idx + self.seq_length + 1]
        
        return torch.tensor(input_seq), torch.tensor(target_seq)

def train():
    print("ğŸš€ å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹...")
    
    # è¶…å‚æ•°
    vocab_size = 1000  # ç®€åŒ–è¯æ±‡è¡¨å¤§å°
    d_model = 128
    n_heads = 4
    d_ff = 512
    n_layers = 2
    seq_length = 20
    batch_size = 32
    num_epochs = 10
    learning_rate = 0.001
    
    # åˆ›å»ºç®€å•æ•°æ®ï¼ˆéšæœºç”Ÿæˆï¼Œç”¨äºæµ‹è¯•ï¼‰
    torch.manual_seed(42)
    data = torch.randint(0, vocab_size, (1000,))  # 1000ä¸ªtoken
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = SimpleDataset(data, None, seq_length)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleTransformer(vocab_size, d_model, n_heads, d_ff, n_layers)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # è®­ç»ƒè®°å½•
    losses = []
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        total_loss = 0
        for i, (inputs, targets) in enumerate(dataloader):
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if i % 10 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(dataloader)}], Loss: {loss.item():.4f}')
        
        avg_loss = total_loss / len(dataloader)
        losses.append(avg_loss)
        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.plot(losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig('results/training_loss.png')
    print("ğŸ“Š è®­ç»ƒå®Œæˆï¼æŸå¤±æ›²çº¿å·²ä¿å­˜åˆ° results/training_loss.png")

if __name__ == "__main__":
    import math
    train()
